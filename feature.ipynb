{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config.dir import DATA_DIR, EM_DATA_DIR ,REPO_DIR\n",
    "from utils.stimulus_utils import load_textgrids, load_simulated_trfiles\n",
    "from utils.textgrid import TextGrid\n",
    "from utils.dsutils import make_semantic_model, make_word_ds, make_phoneme_ds\n",
    "from utils.npp import zscore, mcorr\n",
    "import json\n",
    "from utils.SemanticModel import SemanticModel\n",
    "from utils.interpdata import lanczosinterp2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (0.13.2)\n",
      "Collecting nltools\n",
      "  Using cached nltools-0.5.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting voxelwise_tutorials\n",
      "  Using cached voxelwise_tutorials-0.2.3.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: nibabel>=3.0.1 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nltools) (5.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.0 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nltools) (1.6.1)\n",
      "Collecting nilearn>=0.6.0 (from nltools)\n",
      "  Using cached nilearn-0.12.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "INFO: pip is looking at multiple versions of nltools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting nltools\n",
      "  Downloading nltools-0.5.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading nltools-0.4.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nltools) (1.15.3)\n",
      "Collecting pynv (from nltools)\n",
      "  Using cached pynv-0.3-py3-none-any.whl.metadata (544 bytes)\n",
      "Requirement already satisfied: joblib>=0.15 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nltools) (1.4.2)\n",
      "Collecting deepdish>=0.3.6 (from nltools)\n",
      "  Downloading deepdish-0.3.7-py2.py3-none-any.whl.metadata (856 bytes)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from voxelwise_tutorials) (3.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from voxelwise_tutorials) (3.4.2)\n",
      "Collecting pydot (from voxelwise_tutorials)\n",
      "  Using cached pydot-4.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from voxelwise_tutorials) (3.9.1)\n",
      "Collecting pycortex>=1.2.4 (from voxelwise_tutorials)\n",
      "  Using cached pycortex-1.2.11.tar.gz (37.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting himalaya (from voxelwise_tutorials)\n",
      "  Using cached himalaya-0.4.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pymoten (from voxelwise_tutorials)\n",
      "  Using cached pymoten-0.0.8-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting datalad (from voxelwise_tutorials)\n",
      "  Using cached datalad-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tables in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from deepdish>=0.3.6->nltools) (3.10.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nibabel>=3.0.1->nltools) (4.12.2)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nilearn>=0.6.0->nltools) (5.3.0)\n",
      "Requirement already satisfied: requests>=2.25.0 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nilearn>=0.6.0->nltools) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from pycortex>=1.2.4->voxelwise_tutorials) (72.1.0)\n",
      "Collecting future (from pycortex>=1.2.4->voxelwise_tutorials)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tornado>=4.3 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from pycortex>=1.2.4->voxelwise_tutorials) (6.5.1)\n",
      "Collecting shapely (from pycortex>=1.2.4->voxelwise_tutorials)\n",
      "  Downloading shapely-2.1.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting html5lib (from pycortex>=1.2.4->voxelwise_tutorials)\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numexpr in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from pycortex>=1.2.4->voxelwise_tutorials) (2.11.0)\n",
      "Collecting cython (from pycortex>=1.2.4->voxelwise_tutorials)\n",
      "  Using cached cython-3.1.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: imageio in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from pycortex>=1.2.4->voxelwise_tutorials) (2.37.0)\n",
      "Collecting looseversion (from pycortex>=1.2.4->voxelwise_tutorials)\n",
      "  Using cached looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting mda_xdrlib (from pycortex>=1.2.4->voxelwise_tutorials)\n",
      "  Using cached mda_xdrlib-0.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from requests>=2.25.0->nilearn>=0.6.0->nltools) (2025.7.14)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from scikit-learn>=0.21.0->nltools) (3.5.0)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from datalad->voxelwise_tutorials) (4.3.7)\n",
      "Requirement already satisfied: chardet>=3.0.4 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from datalad->voxelwise_tutorials) (4.0.0)\n",
      "Collecting distro (from datalad->voxelwise_tutorials)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting iso8601 (from datalad->voxelwise_tutorials)\n",
      "  Using cached iso8601-2.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting humanize (from datalad->voxelwise_tutorials)\n",
      "  Using cached humanize-4.12.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting fasteners>=0.14 (from datalad->voxelwise_tutorials)\n",
      "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting patool>=1.7 (from datalad->voxelwise_tutorials)\n",
      "  Using cached patool-4.0.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: tqdm>=4.32.0 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from datalad->voxelwise_tutorials) (4.67.1)\n",
      "Collecting annexremote (from datalad->voxelwise_tutorials)\n",
      "  Using cached annexremote-1.6.6-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting boto3 (from datalad->voxelwise_tutorials)\n",
      "  Downloading boto3-1.40.4-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: keyring!=23.9.0,>=20.0 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from datalad->voxelwise_tutorials) (25.6.0)\n",
      "Collecting keyrings.alt (from datalad->voxelwise_tutorials)\n",
      "  Using cached keyrings.alt-5.0.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: msgpack in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from datalad->voxelwise_tutorials) (1.1.1)\n",
      "Collecting python-gitlab (from datalad->voxelwise_tutorials)\n",
      "  Using cached python_gitlab-6.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: jaraco.classes in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from keyring!=23.9.0,>=20.0->datalad->voxelwise_tutorials) (3.2.1)\n",
      "Requirement already satisfied: jaraco.functools in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from keyring!=23.9.0,>=20.0->datalad->voxelwise_tutorials) (4.1.0)\n",
      "Requirement already satisfied: jaraco.context in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from keyring!=23.9.0,>=20.0->datalad->voxelwise_tutorials) (0.0.0)\n",
      "Collecting botocore<1.41.0,>=1.40.4 (from boto3->datalad->voxelwise_tutorials)\n",
      "  Downloading botocore-1.40.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from boto3->datalad->voxelwise_tutorials) (1.0.1)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->datalad->voxelwise_tutorials)\n",
      "  Using cached s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from html5lib->pycortex>=1.2.4->voxelwise_tutorials) (0.5.1)\n",
      "Requirement already satisfied: more-itertools in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from jaraco.classes->keyring!=23.9.0,>=20.0->datalad->voxelwise_tutorials) (10.3.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nltk->voxelwise_tutorials) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from nltk->voxelwise_tutorials) (2024.11.6)\n",
      "Collecting requests-toolbelt>=1.0.0 (from python-gitlab->datalad->voxelwise_tutorials)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages (from tables->deepdish>=0.3.6->nltools) (9.0.0)\n",
      "Downloading nltools-0.4.7-py2.py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading deepdish-0.3.7-py2.py3-none-any.whl (37 kB)\n",
      "Using cached nilearn-0.12.0-py3-none-any.whl (10.6 MB)\n",
      "Using cached cython-3.1.2-cp312-cp312-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Using cached datalad-1.2.1-py3-none-any.whl (1.3 MB)\n",
      "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading patool-4.0.1-py2.py3-none-any.whl (86 kB)\n",
      "Using cached annexremote-1.6.6-py3-none-any.whl (25 kB)\n",
      "Downloading boto3-1.40.4-py3-none-any.whl (140 kB)\n",
      "Downloading botocore-1.40.4-py3-none-any.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Using cached himalaya-0.4.6-py3-none-any.whl (83 kB)\n",
      "Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Using cached humanize-4.12.3-py3-none-any.whl (128 kB)\n",
      "Using cached iso8601-2.1.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached keyrings.alt-5.0.2-py3-none-any.whl (17 kB)\n",
      "Using cached looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached mda_xdrlib-0.2.0-py3-none-any.whl (14 kB)\n",
      "Using cached pydot-4.0.1-py3-none-any.whl (37 kB)\n",
      "Using cached pymoten-0.0.8-py3-none-any.whl (24 kB)\n",
      "Using cached pynv-0.3-py3-none-any.whl (3.6 kB)\n",
      "Using cached python_gitlab-6.2.0-py3-none-any.whl (144 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading shapely-2.1.1-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: voxelwise_tutorials, pycortex\n",
      "\u001b[33m  DEPRECATION: Building 'voxelwise_tutorials' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'voxelwise_tutorials'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for voxelwise_tutorials (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for voxelwise_tutorials: filename=voxelwise_tutorials-0.2.3-py3-none-any.whl size=30490 sha256=b6472ab5638e3f666f33b21ffafc58f078ff55a630ea6f7e13f8dfb96bf69274\n",
      "  Stored in directory: /Users/genevievelam/Library/Caches/pip/wheels/5e/e5/38/d419a2ce2c1f7862e1657077397c3afdbde004bcf638a87b33\n",
      "  Building wheel for pycortex (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycortex: filename=pycortex-1.2.11-cp312-cp312-macosx_11_0_arm64.whl size=37447500 sha256=ed4a24941b78c7389f47fb89228712ce314ae0fc5ec37f62b487f18d56849661\n",
      "  Stored in directory: /Users/genevievelam/Library/Caches/pip/wheels/ad/bf/da/56c985da6938b8e7848e6e0718dbe7322f9085f5c457442989\n",
      "Successfully built voxelwise_tutorials pycortex\n",
      "Installing collected packages: pymoten, looseversion, annexremote, shapely, pydot, patool, mda_xdrlib, iso8601, humanize, html5lib, future, fasteners, distro, cython, requests-toolbelt, pynv, keyrings.alt, botocore, s3transfer, python-gitlab, pycortex, nilearn, himalaya, deepdish, nltools, boto3, datalad, voxelwise_tutorials\n",
      "\u001b[2K  Attempting uninstall: botocore91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/28\u001b[0m [cython]]\n",
      "\u001b[2K    Found existing installation: botocore 1.39.8━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/28\u001b[0m [cython]\n",
      "\u001b[2K    Uninstalling botocore-1.39.8:[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/28\u001b[0m [botocore]\n",
      "\u001b[2K      Successfully uninstalled botocore-1.39.8m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/28\u001b[0m [botocore]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/28\u001b[0m [voxelwise_tutorials]talad]]r]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.23.2 requires botocore<1.39.9,>=1.39.7, but you have botocore 1.40.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annexremote-1.6.6 boto3-1.40.4 botocore-1.40.4 cython-3.1.2 datalad-1.2.1 deepdish-0.3.7 distro-1.9.0 fasteners-0.19 future-1.0.0 himalaya-0.4.6 html5lib-1.1 humanize-4.12.3 iso8601-2.1.0 keyrings.alt-5.0.2 looseversion-1.3.0 mda_xdrlib-0.2.0 nilearn-0.12.0 nltools-0.4.7 patool-4.0.1 pycortex-1.2.11 pydot-4.0.1 pymoten-0.0.8 pynv-0.3 python-gitlab-6.2.0 requests-toolbelt-1.0.0 s3transfer-0.13.1 shapely-2.1.1 voxelwise_tutorials-0.2.3\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install numpy pandas voxelwise_tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim = 5\n",
    "#ndelays = 4\n",
    "feature =\"eng1000\"\n",
    "subject = 'sub-UTS01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_wordseqs(stories):\n",
    "\tgrids = load_textgrids(stories, DATA_DIR)\n",
    "\twith open( os.path.join(DATA_DIR, \"ds003020/derivative/respdict.json\"), \"r\") as f:\n",
    "\t\trespdict = json.load(f)\n",
    "\ttrfiles = load_simulated_trfiles(respdict)\n",
    "\twordseqs = make_word_ds(grids, trfiles)\n",
    "\treturn wordseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(EM_DATA_DIR, \"sess_to_story.json\"), \"r\") as f:\n",
    "\t\tsess_to_story = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = ['10']#,'11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories, test_stories = [], []\n",
    "for sess in sessions:\n",
    "    stories, tstory = sess_to_story[sess][0], sess_to_story[sess][1]\n",
    "    train_stories.extend(stories)\n",
    "    if tstory not in test_stories:\n",
    "        test_stories.append(tstory)\n",
    "assert len(set(train_stories) & set(test_stories)) == 0, \"Train - Test overlap!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allstories = list(set(train_stories) | set(test_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving encoding model & results to: /Users/genevievelam/Documents/GitHub/stories_fmri/results/eng1000/sub-UTS01\n"
     ]
    }
   ],
   "source": [
    "save_location = os.path.join(REPO_DIR, \"results\",feature, subject)\n",
    "print(\"Saving encoding model & results to:\", save_location)\n",
    "os.makedirs(save_location, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_word_vectors(stories, word_vectors, wordseqs):\n",
    "\t\"\"\"Get Lanczos downsampled word_vectors for specified stories.\n",
    "\n",
    "\tArgs:\n",
    "\t\tstories: List of stories to obtain vectors for.\n",
    "\t\tword_vectors: Dictionary of {story: <float32>[num_story_words, vector_size]}\n",
    "\n",
    "\tReturns:\n",
    "\t\tDictionary of {story: downsampled vectors}\n",
    "\t\"\"\"\n",
    "\tdownsampled_semanticseqs = dict()\n",
    "\tfor story in stories:\n",
    "\t\tdownsampled_semanticseqs[story] = lanczosinterp2D(\n",
    "\t\t\tword_vectors[story], wordseqs[story].data_times, \n",
    "\t\t\twordseqs[story].tr_times, window=3)\n",
    "\treturn downsampled_semanticseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eng1000_vectors(allstories):\n",
    "\t\"\"\"Get Eng1000 vectors (985-d) for specified stories.\n",
    "\n",
    "\tArgs:\n",
    "\t\tallstories: List of stories to obtain vectors for.\n",
    "\n",
    "\tReturns:\n",
    "\t\tDictionary of {story: downsampled vectors}\n",
    "\t\"\"\"\n",
    "\teng1000 = SemanticModel.load(os.path.join(EM_DATA_DIR, \"english1000sm.hf5\"))\n",
    "\twordseqs = get_story_wordseqs(allstories)\n",
    "\tvectors = {}\n",
    "\tfor story in allstories:\n",
    "\t\tsm = make_semantic_model(wordseqs[story], [eng1000], [985])\n",
    "\t\tvectors[story] = sm.data\n",
    "\treturn downsample_word_vectors(allstories, vectors, wordseqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FEATURE_CONFIG = {\n",
    "\t#\"articulation\": get_articulation_vectors,\n",
    "\t#\"phonemerate\": get_phonemerate_vectors,\n",
    "\t#\"wordrate\": get_wordrate_vectors,\n",
    "\t\"eng1000\": get_eng1000_vectors,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_space(feature, *args):\n",
    "\treturn _FEATURE_CONFIG[feature](*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulus & Response parameters:\n",
      "trim: 5, ndelays: 4\n"
     ]
    }
   ],
   "source": [
    "downsampled_feat = get_feature_space(feature, allstories)\n",
    "print(\"Stimulus & Response parameters:\")\n",
    "#print(\"trim: %d, ndelays: %d\" % (trim, ndelays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_serializable(downsampled_feat):\n",
    "    \"\"\"Convert downsampled feature dictionary to a serializable format.\"\"\"\n",
    "    \n",
    "    serializable_dict = downsampled_feat.tolist()\n",
    "\n",
    "    return serializable_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_zscore_and_hrf(stories, downsampled_feat, trim):\n",
    "\t\"\"\"Get (z-scored and delayed) stimulus for train and test stories.\n",
    "\tThe stimulus matrix is delayed (typically by 2,4,6,8 secs) to estimate the\n",
    "\themodynamic response function with a Finite Impulse Response model.\n",
    "\n",
    "\tArgs:\n",
    "\t\tstories: List of stimuli stories.\n",
    "\n",
    "\tVariables:\n",
    "\t\tdownsampled_feat (dict): Downsampled feature vectors for all stories.\n",
    "\t\ttrim: Trim downsampled stimulus matrix.\n",
    "\t\tdelays: List of delays for Finite Impulse Response (FIR) model.\n",
    "\n",
    "\tReturns:\n",
    "\t\tdelstim: <float32>[TRs, features * ndelays]\n",
    "\t\"\"\"\n",
    "\tstim = [zscore(downsampled_feat[s][5+trim:-trim]) for s in stories]\n",
    "\tstim = np.vstack(stim)\n",
    "\t#delays = range(1, ndelays+1)\n",
    "\t#delstim = make_delayed(stim, delays)\n",
    "\treturn stim#delstim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delRstim = apply_zscore_and_hrf(train_stories, downsampled_feat, trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1824, 985)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delRstim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\t\n",
    "import h5py\n",
    "def get_response(stories, subject):\n",
    "\t\"\"\"Get the subject\"s fMRI response for stories.\"\"\"\n",
    "\t#main_path = pathlib.Path(__file__).parent.parent.resolve()\n",
    "\tsubject_x = subject.split('-')[1]\n",
    "\tsubject_dir = os.path.join(DATA_DIR, \"ds003020/derivative/preprocessed_data/%s\" % subject_x)\n",
    "\tbase = subject_dir\n",
    "\tresp = []\n",
    "\tfor story in stories:\n",
    "\t\tresp_path = os.path.join(base, \"%s.hf5\" % story)\n",
    "\t\thf = h5py.File(resp_path, \"r\")\n",
    "\t\tresp.extend(hf[\"data\"][:])\n",
    "\t\thf.close()\n",
    "\treturn np.array(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zRresp:  (1824, 81126)\n"
     ]
    }
   ],
   "source": [
    "# Response\n",
    "zRresp = get_response(train_stories, subject)\n",
    "print(\"zRresp: \", zRresp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zRresp:  (1824, 94251)\n"
     ]
    }
   ],
   "source": [
    "# Response\n",
    "zRresp = get_response(train_stories, 'sub-UTS02')\n",
    "print(\"zRresp: \", zRresp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving encoding model & results to: /Users/genevievelam/Documents/GitHub/stories_fmri/results/eng1000/sub-UTS03\n"
     ]
    }
   ],
   "source": [
    "#save_location = os.path.join(REPO_DIR, \"results\",feature, 'sub-UTS03')\n",
    "#print(\"Saving encoding model & results to:\", save_location)\n",
    "#os.makedirs(save_location, exist_ok=True)\n",
    "\n",
    "#with open(save_location+'/fmri.json', \"w\") as file:\n",
    "#        json.dump(convert_to_serializable(zRresp),file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(save_location+'/features.json', \"w\") as file:\n",
    "#        json.dump(convert_to_serializable(delRstim),file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(save_location+'/fmri.json', \"w\") as file:\n",
    "#        json.dump(convert_to_serializable(zRresp),file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1194863,
     "sourceId": 1997650,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "naturalistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
